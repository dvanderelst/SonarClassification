\relax 
\providecommand\hyper@newdestlabel[2]{}
\providecommand\HyperFirstAtBeginDocument{\AtBeginDocument}
\HyperFirstAtBeginDocument{\ifx\hyper@anchor\@undefined
\global\let\oldcontentsline\contentsline
\gdef\contentsline#1#2#3#4{\oldcontentsline{#1}{#2}{#3}}
\global\let\oldnewlabel\newlabel
\gdef\newlabel#1#2{\newlabelxx{#1}#2}
\gdef\newlabelxx#1#2#3#4#5#6{\oldnewlabel{#1}{{#2}{#3}}}
\AtEndDocument{\ifx\hyper@anchor\@undefined
\let\contentsline\oldcontentsline
\let\newlabel\oldnewlabel
\fi}
\fi}
\global\let\hyper@last\relax 
\gdef\HyperFirstAtBeginDocument#1{#1}
\providecommand\HyField@AuxAddToFields[1]{}
\providecommand\HyField@AuxAddToCoFields[2]{}
\bibstyle{model5-names}
\citation{Barchi2013,VonHelversen2005}
\citation{Vanderelst2016,Vanderelst2017}
\citation{Jensen2005,Yu2019}
\citation{Verboom1999}
\citation{Franz2000}
\citation{Vanderelst2016,Vanderelst2017}
\citation{Yovel2009,Vanderelst2016}
\citation{Vanderelst2015,Vanderelst2016,Steckel2013}
\citation{Simmons1989,Wiegrebe1996,Surlykke1996}
\citation{Geberl2019}
\citation{Warnecke2018}
\citation{Kuc1997b,Kuc1997}
\citation{Vanderelst2016,Vanderelst2017}
\@writefile{toc}{\contentsline {section}{\numberline {1}introduction}{1}{section.1}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {2}Methods}{1}{section.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {2.1}Synthetic echoes and templates}{1}{subsection.2.1}\protected@file@percent }
\citation{Yovel2009}
\citation{Wiegrebe2008}
\citation{Wiegrebe1996,Simmons1989}
\citation{Vanderelst2016}
\citation{Vanderelst2016}
\citation{Steckel2013a}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.2}Natural Templates}{2}{subsection.2.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2.2.1}Acoustic data}{2}{subsubsection.2.2.1}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {1}{\ignorespaces Low dimensional mapping of synthetic echoes. Panels xx-xx, examples of artificial echo sequences. Panels xx-xx, templates derived from each of the echo sequences shown in xx-xx. The templates constructed from the artificial echoes could be projected in a lower dimensional space. A principal component analysis showed that 25\ components captured almost all variance. Panels xx-xx, also depict (in blue) the result of projecting the templates into the 25D space and transforming back to the orginal space. The good fit between the orginal and the reconstructed templates attests to the amount of variance captured by the 25\ principal components.}}{3}{figure.1}\protected@file@percent }
\newlabel{fig:synthetictemplates}{{1}{3}{Low dimensional mapping of synthetic echoes. Panels xx-xx, examples of artificial echo sequences. Panels xx-xx, templates derived from each of the echo sequences shown in xx-xx. The templates constructed from the artificial echoes could be projected in a lower dimensional space. A principal component analysis showed that \pca \ components captured almost all variance. Panels xx-xx, also depict (in blue) the result of projecting the templates into the \pca D space and transforming back to the orginal space. The good fit between the orginal and the reconstructed templates attests to the amount of variance captured by the \pca \ principal components}{figure.1}{}}
\citation{Vanderelst2016}
\citation{Wiegrebe2008}
\citation{Vanderelst2010a,Reijniers2010,Jakobsen2012}
\citation{Vanderelst2016}
\citation{Vanderelst2016}
\@writefile{lof}{\contentsline {figure}{\numberline {2}{\ignorespaces Plots showing the 25\ principal components derived from the templates constructed using synthetic echoes.}}{4}{figure.2}\protected@file@percent }
\newlabel{fig:components}{{2}{4}{Plots showing the \pca \ principal components derived from the templates constructed using synthetic echoes}{figure.2}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2.2.2}Template construction}{4}{subsubsection.2.2.2}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {3}{\ignorespaces Pictures of the sonar data acquisition process. Top: the ensonification device used at the Israeli site at Sde Boker. Bottom: The device used at the Bristol University Royal Fort Gardens. The tape measure indicated the transect along which the device was positioned in 40 steps.}}{4}{figure.3}\protected@file@percent }
\newlabel{fig:datacollection}{{3}{4}{Pictures of the sonar data acquisition process. Top: the ensonification device used at the Israeli site at Sde Boker. Bottom: The device used at the Bristol University Royal Fort Gardens. The tape measure indicated the transect along which the device was positioned in 40 steps}{figure.3}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2.2.3}Noise floor}{4}{subsubsection.2.2.3}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2.2.4}Mapping templates into the 25-dimensional space}{4}{subsubsection.2.2.4}\protected@file@percent }
\citation{Baddeley2012}
\citation{Mcleod1998}
\citation{Kingma2014}
\citation{Ghotra2017}
\citation{Hawkins2004}
\citation{Dau1996}
\citation{Baddeley2012}
\citation{Vanderelst2016}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.3}Artificial Neural Networks}{5}{subsection.2.3}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {4}{\ignorespaces Depiction of the neural network architecture trained for each site.}}{5}{figure.4}\protected@file@percent }
\newlabel{fig:networks}{{4}{5}{Depiction of the neural network architecture trained for each site}{figure.4}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2.3.1}Training examples}{5}{subsubsection.2.3.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {2.4}Perfect Memory}{5}{subsection.2.4}\protected@file@percent }
\citation{Vanderelst2016}
\citation{Vanderelst2016}
\citation{Vanderelst2015a,Mansour2019}
\@writefile{lof}{\contentsline {figure}{\numberline {5}{\ignorespaces Examples of three templates selected from the Israel and Royal Fort data (red). Blue lines depict the result of projecting the templates in the 25-dimensions space and reconstructing by transforming the results back to the original space.}}{6}{figure.5}\protected@file@percent }
\newlabel{fig:templates}{{5}{6}{Examples of three templates selected from the Israel and Royal Fort data (red). Blue lines depict the result of projecting the templates in the \pca -dimensions space and reconstructing by transforming the results back to the original space}{figure.5}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {6}{\ignorespaces Training history of the neural networks. The plots give the value of the loss function as a function of training epoch. Notice the logarithmic x-axis.}}{6}{figure.6}\protected@file@percent }
\newlabel{fig:history}{{6}{6}{Training history of the neural networks. The plots give the value of the loss function as a function of training epoch. Notice the logarithmic x-axis}{figure.6}{}}
\@writefile{toc}{\contentsline {section}{\numberline {3}Results}{6}{section.3}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {4}Discussion}{6}{section.4}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {4.1}The low dimensional world of bats}{6}{subsection.4.1}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {7}{\ignorespaces Classification results for the Israel site. Top row: cumulative error distributions for the azimuth, elevation and location classification. Middle row: Confusion matrices for the classifications. Bottom row: remaining classification entropy for each target (derived from the confusion matrices).}}{7}{figure.7}\protected@file@percent }
\newlabel{fig:israelperformance}{{7}{7}{Classification results for the Israel site. Top row: cumulative error distributions for the azimuth, elevation and location classification. Middle row: Confusion matrices for the classifications. Bottom row: remaining classification entropy for each target (derived from the confusion matrices)}{figure.7}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {8}{\ignorespaces Simular as fig \ref  {fig:israelperformance}, but for the Royal Fort Park site.}}{8}{figure.8}\protected@file@percent }
\newlabel{fig:royalperformance}{{8}{8}{Simular as fig \ref {fig:israelperformance}, but for the Royal Fort Park site}{figure.8}{}}
\citation{Braitenberg2013}
\citation{Williams2000}
\citation{Vanderelst2016,Vanderelst2015a,Mansour2019}
\bibdata{references}
\bibcite{Baddeley2012}{{1}{2012}{{Baddeley et~al.}}{{Baddeley, Graham, Husbands \& Philippides}}}
\bibcite{Barchi2013}{{2}{2013}{{Barchi et~al.}}{{Barchi, Knowles \& Simmons}}}
\bibcite{Braitenberg2013}{{3}{2013}{{Braitenberg \& Sch{\"u}z}}{{}}}
\bibcite{Dau1996}{{4}{1996}{{Dau et~al.}}{{Dau, P{\"u}schel \& Kohlrausch}}}
\bibcite{Franz2000}{{5}{2000}{{Franz \& Mallot}}{{}}}
\bibcite{Geberl2019}{{6}{2019}{{Geberl et~al.}}{{Geberl, Kugler \& Wiegrebe}}}
\bibcite{Ghotra2017}{{7}{2017}{{Ghotra \& Dua}}{{}}}
\bibcite{Hawkins2004}{{8}{2004}{{Hawkins}}{{}}}
\bibcite{Jakobsen2012}{{9}{2012}{{Jakobsen et~al.}}{{Jakobsen, Ratcliffe \& Surlykke}}}
\bibcite{Jensen2005}{{10}{2005}{{Jensen et~al.}}{{Jensen, Moss \& Surlykke}}}
\bibcite{Kingma2014}{{11}{2014}{{Kingma \& Ba}}{{}}}
\bibcite{Kuc1997}{{12}{1997{a}}{{Kuc}}{{}}}
\bibcite{Kuc1997b}{{13}{1997{b}}{{Kuc}}{{}}}
\bibcite{Lin2017a}{{14}{2017}{{Lin et~al.}}{{Lin, Tegmark \& Rolnick}}}
\bibcite{Mansour2019}{{15}{2019}{{Mansour et~al.}}{{Mansour, Koreman, Laurijssen, Steckel, Peremans \& Vanderelst}}}
\bibcite{Mcleod1998}{{16}{1998}{{McLeod et~al.}}{{McLeod, Plunkett \& Rolls}}}
\bibcite{Reijniers2010}{{17}{2010}{{Reijniers et~al.}}{{Reijniers, Vanderelst \& Peremans}}}
\bibcite{Simmons1989}{{18}{1989}{{Simmons et~al.}}{{Simmons, Freedman, Stevenson, Chen \& Wohlgenant}}}
\bibcite{Steckel2013a}{{19}{2013}{{Steckel et~al.}}{{Steckel, Boen \& Peremans}}}
\newlabel{tab:sizes}{{4.2}{9}{Template encoding}{subsection.4.2}{}}
\@writefile{lot}{\contentsline {table}{\numberline {1}{\ignorespaces Table indicating the size of both template data sets and number of weights in the networks. The template size $T$ is obtained as $25\times n$, with $n$ the number of templates in the set. The number of weights $W$ includes all connections between neurons as well as the bias weights for each of the three networks trained. $W (nb)$ omits the bias weights. The ratio $R$ gives the ratio $W/T$.}}{9}{table.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {4.2}Template encoding}{9}{subsection.4.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {4.3}Conclusion}{9}{subsection.4.3}\protected@file@percent }
\bibcite{Steckel2013}{{20}{2013}{{Steckel \& Peremans}}{{}}}
\bibcite{Surlykke1996}{{21}{1996}{{Surlykke \& Bojesen}}{{}}}
\bibcite{Vanderelst2015}{{22}{2015}{{Vanderelst}}{{}}}
\bibcite{Vanderelst2010a}{{23}{2010}{{Vanderelst et~al.}}{{Vanderelst, De~Mey, Peremans, Geipel, Kalko \& Firzlaff}}}
\bibcite{Vanderelst2017}{{24}{2017}{{Vanderelst \& Peremans}}{{}}}
\bibcite{Vanderelst2015a}{{25}{2015}{{Vanderelst et~al.}}{{Vanderelst, Peremans \& Holderied}}}
\bibcite{Vanderelst2016}{{26}{2016}{{Vanderelst et~al.}}{{Vanderelst, Steckel, Boen, Peremans \& Holderied}}}
\bibcite{Verboom1999}{{27}{1999}{{Verboom et~al.}}{{Verboom, Boonman \& Limpens}}}
\bibcite{VonHelversen2005}{{28}{2005}{{Von~Helversen \& Winter}}{{}}}
\bibcite{Warnecke2018}{{29}{2018}{{Warnecke et~al.}}{{Warnecke, Mac{\'i}as, Falk \& Moss}}}
\bibcite{Wiegrebe2008}{{30}{2008}{{Wiegrebe}}{{}}}
\bibcite{Wiegrebe1996}{{31}{1996}{{Wiegrebe \& Schmidt}}{{}}}
\bibcite{Williams2000}{{32}{2000}{{Williams}}{{}}}
\bibcite{Yovel2009}{{33}{2009}{{Yovel et~al.}}{{Yovel, Stilz, Franz, Boonman \& Schnitzler}}}
\bibcite{Yu2019}{{34}{2019}{{Yu et~al.}}{{Yu, Luo, Wohlgemuth \& Moss}}}
